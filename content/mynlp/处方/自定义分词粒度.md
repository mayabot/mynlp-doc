---
title: 自定义分词粒度
draft: false
weight: 1000
---

这个是关于分词粒度的问题，很难满足所有场景的需求。

下面举例日期词粒度问题：

默认2019年9月13日被处理为一个词，有些场景需要进一步切分

一：不加任何干预的情况
```kotlin
val lexer = Lexers.core()
    println(lexer.scan("今年的2019年9月13日，中秋节快乐"))
    println(lexer.scan("2018年12月11日"))
```
同样输出日期整体，这是由于atom分词算法决定的。
```txt
今年/t 的/u 2019年9月13日/t ,/w 中秋节/t 快乐/a
2018年12月11日/t
```

二：如果你业务上需要对日期类型进行细粒度控制
自行编写插件

```kotlin
fun main() {

    val builder = Lexers.coreBuilder().withPos()

   // 安装插件
    builder.with(PipelineLexerPlugin {  builder ->
        builder.addProcessor(SplitDateProcessor())
    })

    val lexer = builder.build()

    println(lexer.scan("今年的2019年9月13日，中秋节快乐"))
    println(lexer.scan("2018年12月11日"))

}

class SplitDateProcessor :BaseSegmentComponent(LEVEL3) , WordpathProcessor{

    val pattern = Pattern.compile("(\\d{4}年)(\\d{1,2}月)(\\d{1,2}日)")

    override fun process(wordPath: Wordpath?): Wordpath {
        val wp = wordPath!!
        wordPath.iteratorVertex().forEach { vertex ->
            val word = vertex.realWord()
            if(word.length>=9){
                val matcher = pattern.matcher(word)
                if (matcher.matches()) {
                    val start = vertex.offset()
                    val l1 = matcher.group(1).length
                    val l2 = matcher.group(2).length
                    val l3 = matcher.group(3).length
                    wp.combine(start,l1)
                    wp.combine(start + l1,l2)
                    wp.combine(start + l1 + l2,l3)
                }
            }
        }
        return wp
    }
}
```

输出：
```txt
今年/t 的/u 2019年/t 9月/t 13日/t ,/w 中秋节/t 快乐/a
2018年/t 12月/t 11日/t
```

